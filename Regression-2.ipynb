{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdfca6d6-2fae-4ba1-93a9-40c5946cfab6",
   "metadata": {},
   "source": [
    "### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "\n",
    "- **R-squared (RÂ²)** is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variables. It indicates how well the model explains the variability of the target variable.\n",
    "- **Formula**:  \n",
    "  \\[\n",
    "  R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n",
    "  \\]\n",
    "  Where:\n",
    "  - \\( SS_{res} \\) is the sum of squared residuals.\n",
    "  - \\( SS_{tot} \\) is the total sum of squares (variance of the data).\n",
    "- **Interpretation**: An \\( R^2 \\) value of 1 means the model explains all the variability, while 0 means it explains none of the variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b439e0-58e7-4822-beff-e2f94828caad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59efa097-cf73-4c03-a4ad-2bfbd1053764",
   "metadata": {},
   "source": [
    "### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "- **Adjusted R-squared** is a modified version of \\( R^2 \\) that adjusts for the number of predictors in the model. It penalizes adding more variables that do not improve the model.\n",
    "- **Formula**:  \n",
    "  \\[\n",
    "  R^2_{adj} = 1 - \\left( \\frac{1 - R^2}{n - p - 1} \\right)\n",
    "  \\]\n",
    "  Where:\n",
    "  - \\( n \\) is the number of data points.\n",
    "  - \\( p \\) is the number of predictors.\n",
    "- **Difference**: Regular \\( R^2 \\) can increase with the addition of irrelevant predictors, while adjusted \\( R^2 \\) accounts for the number of predictors and only increases if the new variable improves the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52652c90-e516-4f67-a425-1311fa831842",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29a6a79f-19b0-45a3-a11f-8e48db17b2c7",
   "metadata": {},
   "source": [
    "### Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "- **Adjusted R-squared** is more appropriate when comparing models with different numbers of independent variables. It helps to avoid overfitting by penalizing the inclusion of unnecessary variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652125b-f219-446f-8687-089d9055163d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb2f40e1-0010-4ea6-8fdd-4dbf568106a6",
   "metadata": {},
   "source": [
    "### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "\n",
    "- **RMSE (Root Mean Squared Error)**:  \n",
    "  \\[\n",
    "  RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}\n",
    "  \\]\n",
    "  - Represents the standard deviation of residuals and measures how well the model predicts actual values.\n",
    "\n",
    "- **MSE (Mean Squared Error)**:  \n",
    "  \\[\n",
    "  MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
    "  \\]\n",
    "  - Measures the average squared difference between predicted and actual values.\n",
    "\n",
    "- **MAE (Mean Absolute Error)**:  \n",
    "  \\[\n",
    "  MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}|\n",
    "  \\]\n",
    "  - Measures the average absolute difference between predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26003c18-897d-4220-8f2a-a427953d4291",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c35fed5-7205-4dc3-8f5a-36e6584a5810",
   "metadata": {},
   "source": [
    "### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "\n",
    "- **RMSE**:\n",
    "  - **Advantages**: Penalizes large errors more than small ones, making it sensitive to outliers.\n",
    "  - **Disadvantages**: Difficult to interpret in terms of the original units of the data.\n",
    "\n",
    "- **MSE**:\n",
    "  - **Advantages**: Similar to RMSE but simpler to compute and penalizes larger errors.\n",
    "  - **Disadvantages**: Like RMSE, it penalizes outliers and is not in the same units as the original data.\n",
    "\n",
    "- **MAE**:\n",
    "  - **Advantages**: Provides a more interpretable measure in terms of original units and is less sensitive to outliers.\n",
    "  - **Disadvantages**: Does not penalize large errors as heavily as RMSE or MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9acb1f-c04d-4711-9545-b141be7b21ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "078aea2f-dcb4-4348-afe2-89c7fcbcf183",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "\n",
    "- **Lasso Regularization** (Least Absolute Shrinkage and Selection Operator) adds a penalty to the absolute values of the coefficients, shrinking some of them to exactly zero. This allows Lasso to perform feature selection by excluding irrelevant features.\n",
    "  - **Formula**:  \n",
    "    \\[\n",
    "    L(\\beta) = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "    \\]\n",
    "- **Ridge Regularization** penalizes the squared values of the coefficients and keeps all variables in the model but shrinks them towards zero.\n",
    "  - **Formula**:  \n",
    "    \\[\n",
    "    L(\\beta) = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
    "    \\]\n",
    "- **When to use**: Lasso is better when you want to perform feature selection, while Ridge is preferable when all variables are relevant, but you want to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e140a-3437-4173-8787-c3c083512b67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cbecf5d-e706-48e3-b516-b126f35c1716",
   "metadata": {},
   "source": [
    "### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "\n",
    "- **Regularized models** add a penalty term (either L1 or L2) to the cost function, discouraging complex models with large coefficients. This prevents the model from fitting the noise in the training data, which can lead to overfitting.\n",
    "  - **Example**: In a model predicting housing prices, adding too many irrelevant features (like house color) may lead to overfitting. Regularization ensures that such irrelevant features have little or no effect by shrinking their coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac9b7a8-8619-4bb3-9ad2-bc867c9d28f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da0408f8-67b7-4e4e-8845-44b53dd3e23a",
   "metadata": {},
   "source": [
    "### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "\n",
    "- **Limitations**:\n",
    "  - Regularized models can shrink coefficients too much, leading to **underfitting** if the regularization parameter is too high.\n",
    "  - Regularization assumes all features contribute equally to the outcome, which may not hold in all datasets.\n",
    "  - Lasso may arbitrarily exclude some important variables when multicollinearity exists.\n",
    "  - Regularization may not perform well in non-linear relationships unless combined with feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e8a6a-94ff-48ff-b1f3-584ed3cbadbb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95a62d38-2282-45c9-8030-1e397d0041e3",
   "metadata": {},
   "source": [
    "### Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "- **Choice**: Model B, with an MAE of 8, appears better as it indicates a smaller average error.\n",
    "- **Limitations**: MAE does not penalize large errors as much as RMSE. If large errors are important to your analysis, you might prefer Model A's RMSE metric. The choice of metric depends on the nature of the problem and whether large errors are more costly than smaller ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16287de5-d964-4947-81a3-7b67d0eb246e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62dbe793-83f9-408e-aa8a-922137d00acc",
   "metadata": {},
   "source": [
    "### Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
    "\n",
    "- **Choice**: It depends on the context:\n",
    "  - If feature selection is important, choose **Model B** (Lasso) as it tends to exclude irrelevant features by setting coefficients to zero.\n",
    "  - If the goal is to prevent overfitting while retaining all variables, choose **Model A** (Ridge).\n",
    "  \n",
    "- **Trade-offs**: Lasso may eliminate important features in the presence of multicollinearity, while Ridge shrinks coefficients without eliminating variables, making it more robust for highly correlated data. The regularization parameter also affects the degree of shrinkage, so choosing an optimal parameter is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fecf364-9c4c-455a-b1d9-8b486c5d6e4d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
